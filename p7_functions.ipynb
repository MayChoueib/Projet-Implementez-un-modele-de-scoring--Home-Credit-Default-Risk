{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35213766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d788fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Transformations de variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder#, RobustScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, f1_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Metrics de ML\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict, StratifiedKFold, GridSearchCV, KFold, train_test_split\n",
    "\n",
    "from sklearn.base import clone\n",
    "# Packages hyperopt pour la séléction d'hyperparamètres\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe43b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compar_col(df1, df2, name1='df1', name2='df2'):\n",
    "    ''' Cette fonction compare les colonnes de 2 dataframes et sort une liste des colonnes differentes pour chaque dataframe'''\n",
    "   \n",
    "    col_1 = list(df1.columns) \n",
    "    col_2 = list(df2.columns)\n",
    "    diff_col_1=[]\n",
    "    diff_col_2=[]\n",
    "    same_col=[]\n",
    "    \n",
    "   \n",
    "    for col in col_1:\n",
    "        if col in col_2:\n",
    "            same_col.append(col)\n",
    "        else:\n",
    "            diff_col_1.append(col)\n",
    "    for col in col_2:\n",
    "        if col not in col_1:\n",
    "            diff_col_2.append(col)   \n",
    "    df1.name = 'df1'\n",
    "    print(f\"Variables differentes dans {name1} : {diff_col_1} \\n Variables differentes dans {name2} : {diff_col_2} \") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bd5abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_train_test(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Merge les jeu d'entrainement et de test en rajoutant\n",
    "    une colonne 'Test' qui prend les valeurs True ou False\n",
    "    df_train(pd.DataFrame): jeu d'entrainement\n",
    "    df_test(pd.DataFrame): jeu test\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['Test'] = False\n",
    "    df_test['Test'] = True\n",
    "    df_test['TARGET'] = np.nan\n",
    "    df_app_tot = df_train.append( df_test,\n",
    "                           ignore_index=True,\n",
    "                           sort=False)\n",
    "    return df_app_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31dcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour comparer valeurs entre train et test sets\n",
    "def display_uniq_cat(df):\n",
    "    \"\"\"\n",
    "    Pour chaque colonne catégorielle, indique le nombre et le nom\n",
    "    des différentes valeurs prises\n",
    "    \"\"\"\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            print(col)\n",
    "            print('Number Unique in Train:', df[~df['Test']][col].nunique())\n",
    "            print('Number Unique in Test: ', df[df['Test']][col].nunique())\n",
    "            print('Unique in Train: ', sorted([str(element) for element in\n",
    "                          df[~df['Test']][col].unique().tolist()]))\n",
    "            print('Unique in Test: ', sorted([str(element) for element in\n",
    "                          df[df['Test']][col].unique().tolist()]))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6013f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder les variables categorielles \n",
    "def cat_encoder(app):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encode les variables catégorielles avec un label encoder ou un one hot encoding suivant\n",
    "    le nombre de valeurs différentes prises par chaque variable    \n",
    "    \"\"\"\n",
    "\n",
    "    # Label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # categorials features to one hot encode\n",
    "    cat_features = []\n",
    "    \n",
    "    \n",
    "    for col in app: \n",
    "       \n",
    "        if (col != 'Test' and col != 'TARGET'):\n",
    "            if app[col].dtype == object: \n",
    "                # Label encode binary fearures in training set\n",
    "                if app[col].nunique() <= 2:\n",
    "                    app[col] = label_encoder.fit_transform(app[col])\n",
    "                # get colunms to one hot encode\n",
    "                elif app[col].nunique() > 2:\n",
    "                     cat_features.append(col)\n",
    "\n",
    "    # One-hot encode categorical features in train set\n",
    "    app = pd.get_dummies(app, columns=cat_features)\n",
    "       \n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d97b7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value retourne df avec pourcentage\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff7b2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plt_cat_var(data, column_name, figsize = (18,6), percentage_display = True, plot_defaulter = True, rotation = 0,\n",
    "                                   horizontal_adjust = 0, fontsize_percent = 'x-small'):\n",
    "    \n",
    "    plt.figure(figsize = figsize, tight_layout = False)\n",
    "    sns.set(style = 'whitegrid', font_scale = 1.2)\n",
    "    \n",
    "    #plotting overall distribution of category\n",
    "    plt.subplot(1,2,1)\n",
    "    data_to_plot = data[column_name].value_counts().sort_values(ascending = False)\n",
    "    ax = sns.barplot(x = data_to_plot.index, y = data_to_plot, palette = 'Set1')\n",
    "    \n",
    "    if percentage_display:\n",
    "        total_datapoints = len(data[column_name].dropna())\n",
    "        for p in ax.patches:\n",
    "            ax.text(p.get_x() + horizontal_adjust, p.get_height() + 0.005 * total_datapoints, '{:1.02f}%'.format(p.get_height() * 100 / total_datapoints), fontsize = fontsize_percent)\n",
    "        \n",
    "    plt.xlabel(column_name, labelpad = 10)\n",
    "    plt.title(f'Distribution of {column_name}', pad = 20)\n",
    "    plt.xticks(rotation = rotation)\n",
    "    plt.ylabel('Counts')\n",
    "    \n",
    "    #plotting distribution of category for Defaulters\n",
    "    if plot_defaulter:\n",
    "        percentage_defaulter_per_category = (data[column_name][data.TARGET == 1].value_counts() * 100 / data[column_name].value_counts()).dropna().sort_values(ascending = False)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        sns.barplot(x = percentage_defaulter_per_category.index, y = percentage_defaulter_per_category, palette = 'Set2')\n",
    "        plt.ylabel('Pourcentage des clients non-solvables par categorie')\n",
    "        plt.xlabel(column_name, labelpad = 10)\n",
    "        plt.xticks(rotation = rotation)\n",
    "        plt.title(f'Pourcentage des clients non-solvables par categorie {column_name}', pad = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4077fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, name):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df : dataframe\n",
    "            feature importances. Must have the features in a column\n",
    "            called `features` and the importances in a column called `importance\n",
    "        \n",
    "    Return\n",
    "    -------\n",
    "        shows a plot of the 15 most importance features\n",
    "        \n",
    "        df : dataframe\n",
    "            feature importances sorted by importance (highest to lowest) \n",
    "            with a column for normalized importance\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (14, 10))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title(f'Feature Importances_{name}')\n",
    "    \n",
    "    \n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124019b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui change type de variable \n",
    "def change_var_type(df, var_type_keep, new_var_type):\n",
    "    for col in df:\n",
    "        if df[col].dtype not in var_type_keep:  \n",
    "              df[col] = df[col].astype(new_var_type)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction test complet de kaggle Mr. KOEHRSEN\n",
    "\n",
    "def model(features, test_features, n_folds = 5):\n",
    "\n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "\n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        n_folds (int, default = 5): number of folds to use for cross validation\n",
    "\n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "     # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 0)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = LGBMClassifier(n_estimators=10000, boosting_type = 'goss',\n",
    "                   objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 0)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'],\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "   # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "\n",
    "    return submission, feature_importances, metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b10643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour recuperer train de test app\n",
    "\n",
    "def get_train(df):\n",
    "    train_set = df.loc[~df[\"Test\"], :]\n",
    "    test_set =  df.loc[df[\"Test\"], :]\n",
    "    train_set.drop(columns=[\"Test\"], inplace=True)\n",
    "    test_set.drop(columns=[\"Test\", \"TARGET\"], inplace=True)\n",
    "    \n",
    "    return train_set, test_set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281963a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_columns(train, test, threshold = 90):\n",
    "    # Calculate missing stats for train and test (remember to calculate a percent!)\n",
    "    train_miss = pd.DataFrame(train.isnull().sum())\n",
    "    train_miss['percent'] = 100 * train_miss[0] / len(train)\n",
    "    \n",
    "    test_miss = pd.DataFrame(test.isnull().sum())\n",
    "    test_miss['percent'] = 100 * test_miss[0] / len(test)\n",
    "    \n",
    "    # list of missing columns for train and test\n",
    "    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n",
    "    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n",
    "    \n",
    "    # Combine the two lists together\n",
    "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
    "    \n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    train = train.drop(columns = missing_columns)\n",
    "    test = test.drop(columns = missing_columns)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c043fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction model avec split train et AUC test\n",
    "\n",
    "def model_red(features, test_features, n_folds=5):   #test_features, model,\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "     # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    labels_test = test_features['TARGET']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "      \n",
    "            \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Create the model\n",
    "        model = LGBMClassifier(n_estimators=10000, boosting_type = 'goss',\n",
    "                   objective = 'binary',# is_unbalance = True,\n",
    "                                    learning_rate = 0.05, class_weight = 'balanced',\n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 0)\n",
    "\n",
    "\n",
    "#       model = GridSearchCV(estimator=model, param_grid=xgb_grid)       \n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'],\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1]/ k_fold.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, \n",
    "                                                         num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "#     predictions = model.predict_proba(test_features)[:,1]\n",
    "    roc = roc_auc_score(labels_test, test_predictions)\n",
    "    print(f\"AUC-Test: {roc:.5f}\")    \n",
    "    # Make the submission dataframe\n",
    "    #submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "   # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "# \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "\n",
    "    return roc,  feature_importances, metrics    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c871d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imb_pipeline(mod, name_mod, features, test_features, params):\n",
    "    scores = []\n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "     # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    labels_test = test_features['TARGET']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    start_time = time.time()\n",
    "   \n",
    "    model = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('sampling', SMOTE(random_state=0)),\n",
    "        ('classification', mod)\n",
    "        \n",
    "    ])\n",
    "\n",
    "    score={'AUC':'roc_auc', \n",
    "           #'RECALL':'recall',\n",
    "           #'PRECISION':'precision',\n",
    "           'F1':'f1'}\n",
    "\n",
    "    gcv = GridSearchCV(estimator=model,  param_grid=params, cv=5, scoring=score, n_jobs=-1, refit='AUC', #\n",
    "                       return_train_score=True)   \n",
    "    gcv.fit(features, labels)\n",
    "    \n",
    "    \n",
    "    test_predictions = gcv.predict_proba(test_features)[:,1]\n",
    "    roc_test = roc_auc_score(labels_test, test_predictions)\n",
    "    mean_best_valid =  gcv.best_score_\n",
    "   \n",
    "    scores.append([name_mod, mean_best_valid, roc_test, (time.time() - start_time)])\n",
    "    \n",
    "\n",
    "    best_model = clone(gcv.best_estimator_)\n",
    "    print(f\"{gcv.best_estimator_}\")\n",
    "    print(f\"{gcv.cv_results_}\")\n",
    "    \n",
    "    return gcv, scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c55860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imb_best_class(model, name_mod, features, test_features):\n",
    "    scores = []\n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "     # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    labels_test = test_features['TARGET']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    start_time = time.time()\n",
    "   \n",
    "    score={'AUC':'roc_auc', \n",
    "           #'RECALL':'recall',\n",
    "           #'PRECISION':'precision',\n",
    "           'F1':'f1'}\n",
    "    \n",
    "    cv = cross_validate(model, features, labels, cv=5, scoring='roc_auc', n_jobs=-1,\n",
    "                       return_train_score=True)\n",
    "    scores = cross_val_score(model, features, labels, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    mean_score = scores.mean()\n",
    "                       \n",
    "\n",
    "    return cv, scores, mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "def model_v1(predictor, features, eval_metric, cv=skf):\n",
    "    \n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features.index\n",
    "\n",
    "    # Extract the targets for training\n",
    "    targets = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Apply the pipeline\n",
    "#     features = pipeline.fit_transform(features)\n",
    "    targets = np.array(targets)    \n",
    "\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('sampling', SMOTE(random_state=0)),\n",
    "        ('classification', predictor)\n",
    "        \n",
    "    ])\n",
    "\n",
    "\n",
    "    def optim_score(params,\n",
    "                    #model=predictor,\n",
    "                    x_train=features,\n",
    "                    y_train=targets,\n",
    "                    cv=cv,\n",
    "                    eval_metric=eval_metric):\n",
    "\n",
    "        # the function gets a set of variable parameters in \"param\"\n",
    "        params_model = {'classification__n_estimators': int(params['classification__n_estimators']), \n",
    "                        'classification__max_depth': int(params['classification__max_depth']), \n",
    "                        'classification__learning_rate': params['classification__learning_rate'],\n",
    "                        'classification__subsample': params['classification__subsample'],\n",
    "                        'classification__colsample_bytree': params['classification__colsample_bytree'],\n",
    "                        'classification__num_leaves': int(params['classification__num_leaves']),\n",
    "                       \n",
    "        }\n",
    "\n",
    "        # asssigne les paramètres du modèle\n",
    "        model.set_params(**params_model)\n",
    "        \n",
    "        # Seuil de solvabilité\n",
    "        threshold = params['classification__solvability_threshold']\n",
    "\n",
    "        # Cross-validation à 5 passes : retourne le score de probabilité\n",
    "        y_proba = cross_val_predict(model,\n",
    "                                    x_train,\n",
    "                                    y_train,\n",
    "                                    method='predict_proba',\n",
    "                                    cv=cv, n_jobs=1)[:, 1]\n",
    "       \n",
    "        # Si proba > seuil alors la prédiction est positive : 1\n",
    "        y_pred = (y_proba > threshold)\n",
    "        y_pred = np.array(y_pred > 0) * 1\n",
    "        \n",
    "        # Calcul du score suivant la métrique utilisé\n",
    "        score = eval_metric(y_train, y_pred)\n",
    "        # if loss, Since we have to minimize the score, we return 1- score.\n",
    "        return 1 - score # Retourne le score\n",
    "\n",
    " \n",
    "    # possible values of parameters\n",
    "    space={'classification__n_estimators': hp.quniform('classification__n_estimators', 200, 2000, 200),\n",
    "           'classification__max_depth' : hp.quniform('classification__max_depth', 2, 30, 2),\n",
    "           'classification__learning_rate': hp.loguniform('classification__learning_rate', np.log(0.005), np.log(0.2)),\n",
    "           'classification__subsample': hp.quniform('classification__subsample', 0.1, 1.0, 0.2),\n",
    "          'classification__colsample_bytree': hp.quniform('classification__colsample_by_tree', 0.6, 1.0, 0.1),\n",
    "           'classification__num_leaves': hp.quniform('classification__num_leaves', 4, 100, 4),\n",
    "           'classification__solvability_threshold': hp.quniform('classification__solvability_threshold', 0.0, 1.0, 0.025)\n",
    "    }\n",
    "\n",
    "\n",
    "    best=fmin(fn=optim_score, # function to optimize\n",
    "              space=space, \n",
    "              algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n",
    "              max_evals=10, # maximum number of iterations\n",
    "    )\n",
    "\n",
    "    # computing the score on the test set\n",
    "    model.set_params(\n",
    "                   classification__n_estimators=int(best['classification__n_estimators']),\n",
    "                   classification__max_depth=int(best['classification__max_depth']),\n",
    "                   classification__learning_rate=best['classification__learning_rate'],\n",
    "                   classification__subsample=best['classification__subsample'],           \n",
    "                   classification__colsample_bytree=best['classification__colsample_by_tree'],\n",
    "                   classification__num_leaves=int(best['classification__num_leaves']),\n",
    "          \n",
    "    )\n",
    "    \n",
    "    # Entrainement du modèle sur tout le jeux de données\n",
    "    model.fit(features, targets)\n",
    "\n",
    "    # Record the best parameters\n",
    "    best_parameters = best\n",
    "    \n",
    "    # Record the feature importances\n",
    "    feature_importance_values = model.steps[2][1].feature_importances_\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': feature_importance_values})\n",
    "\n",
    "    return best_parameters, feature_importances, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(model, features, test_features, n_folds=5):   \n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "     # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    labels_test = test_features['TARGET']\n",
    "\n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "      \n",
    "            \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "\n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'],\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1]/ k_fold.n_splits\n",
    "\n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "\n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "#     predictions = model.predict_proba(test_features)[:,1]\n",
    "    roc = roc_auc_score(labels_test, test_predictions)\n",
    "    print(f\"AUC-Test: {roc:.5f}\")    \n",
    "\n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "   # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "# \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "\n",
    "    return roc, feature_importances, metrics    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94759664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_metier(y_true, y_pred, fn_value=-10, fp_value=0, tp_value=0, tn_value=1):\n",
    "\n",
    "    # Matrice de Confusion\n",
    "    mat_conf = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Nombre de True Negatifs\n",
    "    tn = mat_conf[0, 0]\n",
    "    # Nombre de Faux Négatifs\n",
    "    fn = mat_conf[1, 0]\n",
    "    # Nombre de Faux Positifs\n",
    "    fp = mat_conf[0, 1]\n",
    "    # Nombre de True Positifs\n",
    "    tp = mat_conf[1, 1]\n",
    "    \n",
    "    # Gain total\n",
    "    J = tp*tp_value + tn*tn_value + fp*fp_value + fn*fn_value\n",
    "    \n",
    "    # Gain maximum\n",
    "    max_J = (fp + tn)*tn_value + (fn + tp)*tp_value\n",
    "    \n",
    "    # Gain minimum\n",
    "    min_J = (fp + tn)*fp_value + (fn + tp)*fn_value\n",
    "    \n",
    "    # Gain normalisé entre 0 et 1\n",
    "    J_normalized = (J - min_J)/(max_J - min_J)\n",
    "    return J_normalized  # Retourne la fonction d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusion\n",
    "#Fonction permettant d'afficher plusieurs matrices de confusions en fonctions des différentes prédictions\n",
    "\n",
    "def plot_heatmap_confusion(y_valid, **y_valid_pred):\n",
    "    \n",
    "    plt.figure(1, figsize=(12, 4))\n",
    "    results = {}\n",
    "    \n",
    "    j = 1\n",
    "    for i, y_pred in y_valid_pred.items():\n",
    "        \n",
    "        plt.subplot(1, len(y_valid_pred), j)\n",
    "\n",
    "        conf_mx = confusion_matrix(y_valid, y_pred)\n",
    "        results[i] = conf_mx\n",
    "        mat_conf_df = pd.DataFrame(conf_mx,\n",
    "                                   columns=[\"Solvable\",\"Non Solvable\"],\n",
    "                                   index=[\"Solvable\", \"Non Solvable\"])\n",
    "        \n",
    "        sns.heatmap(mat_conf_df, annot=True, linewidths=.7, fmt='g')\n",
    "\n",
    "        plt.title(i)\n",
    "        plt.ylim(0, 2)\n",
    "        plt.xlabel(\"Classes prédites\")\n",
    "        plt.ylabel(\"Classes réelles\")\n",
    "        j+=1\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b643d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution du gain en fonction du seuil de solvabilité\n",
    "def gain_seuil(clf, X, y):\n",
    "    \n",
    "    cost_function = []\n",
    "    threshold_x = np.linspace(0.0, 1, 20)\n",
    "    \n",
    "    for threshold in threshold_x:\n",
    "        \n",
    "        # Score du modèle : 0 à 1\n",
    "        y_scores = clf.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Score > seuil de solvabilité : retourne 1 sinon 0\n",
    "        y_pred = (y_scores > threshold)\n",
    "        \n",
    "        y_pred = np.array(y_pred > 0) * 1\n",
    "        # Calcul de l'indice bancaire\n",
    "        cost_function.append(metric_metier(y, y_pred))\n",
    "        \n",
    "    # Affichage du gain en fonction du seuil de solvabilité    \n",
    "    plt.plot(threshold_x, cost_function)\n",
    "    plt.xlabel(\"Seuil de probabilité\")\n",
    "    plt.ylabel(\"Indice banquaire\")\n",
    "    plt.xticks(np.linspace(0.1, 1, 10))\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3217ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(classifiers, X, y, n_splits=5, fit=True):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits)    \n",
    "    for name_clf, clf in classifiers.items():\n",
    "        print(name_clf)\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fp_rate = np.linspace(0, 1, 100)\n",
    "        for i, (train, test) in enumerate(kf.split(X, y)):\n",
    "\n",
    "            full_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "          ('sampling', SMOTE()),\n",
    "          ('classification', clf)\n",
    "        #,('scaler', RobustScaler())\n",
    "         ])\n",
    "            \n",
    "            if fit :\n",
    "                full_pipeline.fit(X.iloc[train, :], y.iloc[train])\n",
    "\n",
    "            y_pred_grd = full_pipeline.predict_proba(X.iloc[test, :])[:, 1]\n",
    "            fp_rate, tp_rate, tresholds = roc_curve(y.iloc[test], y_pred_grd)\n",
    "\n",
    "            interp_tp_rate = np.interp(mean_fp_rate, fp_rate, tp_rate)\n",
    "            interp_tp_rate[0] = 0.0\n",
    "            tprs.append(interp_tp_rate)\n",
    "            auc_ = auc(fp_rate, tp_rate)\n",
    "\n",
    "            aucs.append(auc_)\n",
    "\n",
    "        mean_tp_rate = np.mean(tprs, axis=0)\n",
    "        mean_tp_rate[-1] = 1.0\n",
    "        mean_auc = auc(mean_fp_rate, mean_tp_rate)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fp_rate, mean_tp_rate,\n",
    "            label=name_clf + r'(AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2,\n",
    "            alpha=.8)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=15)\n",
    "    plt.ylabel('True Positive Rate', fontsize=15)\n",
    "    plt.title('Receiver operating characteristic example', fontsize=20)\n",
    "    plt.legend(loc=\"lower right\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b0263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
